
import os
import re
import mobi
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import shutil


# å®šä¹‰åƒåœ¾æ–‡æœ¬çš„æ­£åˆ™æ¨¡å¼ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•ï¼‰
BAD_PATTERNS = [
    r"æœ¬ä¹¦ç”±.*æ•´ç†", 
    r"åŠ .*å¾®ä¿¡", 
    r"QQï¼š?\d+", 
    r"å¾®ä¿¡å…¬ä¼—å·", 
    r"ä¸‹è½½ç½‘ç«™", 
    r"www\..*", 
    r"http[s]?://.*", 
    # r"å°ç¼–", 
]

def is_bad_text(text: str) -> bool:
    """åˆ¤æ–­ä¸€æ®µæ–‡å­—æ˜¯å¦å±äºåƒåœ¾å¹¿å‘Š"""
    for pat in BAD_PATTERNS:
        if re.search(pat, text):
            return True
    return False


# ä¸­æ–‡æ ‡ç‚¹ -> è‹±æ–‡æ ‡ç‚¹æ˜ å°„
punct_map = {
    'ï¼Œ': ',',
    'ã€‚': '.',
    'ï¼': '!',
    'ï¼Ÿ': '?',
    'ï¼š': ':',
    'ï¼›': ';',
    'ï¼ˆ': '(',
    'ï¼‰': ')',
    'ã€': '[',
    'ã€‘': ']',
    'â€œ': '"',
    'â€': '"',
    'â€˜': "'",
    'â€™': "'",
    'ã€': ',',
    'ã€Š': '<',
    'ã€‹': '>',
}

# ç‰¹æ®Šæƒ…å†µ: "â€”â€”" å’Œ "â€¦" éœ€è¦å•ç‹¬å¤„ç†
def normalize_punctuation(text: str) -> str:
    # å…ˆæ›¿æ¢å¤šå­—ç¬¦æ ‡ç‚¹
    text = text.replace("â€”â€”", "--").replace("â€¦", "...")
    
    # å•å­—ç¬¦æ˜ å°„ç”¨ translate
    trans_table = str.maketrans(punct_map)
    text = text.translate(trans_table)

    # åˆ é™¤éä¸­è‹±æ–‡å’Œæ•°å­—çš„å­—ç¬¦ï¼Œæ›¿æ¢ä¸ºç©ºæ ¼
    text = re.sub(r"[^\u4e00-\u9fa5a-zA-Z0-9\s\.,!?;:()\-â€”\"'<>]", " ", text)

    return text

def read_epub(file_path):
    """è§£æ EPUBï¼ŒæŒ‰ç« èŠ‚è¾“å‡ºï¼Œè¿”å› chapters åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (title, content)"""
    book = epub.read_epub(file_path)
    print(f"=== {os.path.basename(file_path)} ===")

    chapters = []
    current_chapter_title = None
    current_chapter_content = []

    # EPUB æ²¡æœ‰æ­£æ–‡é‡Œçš„ TOCï¼Œè¿™é‡Œ toc_texts ç›´æ¥è®¾ä¸ºç©ºé›†åˆå³å¯
    # toc_texts = set()

    for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):
        soup = BeautifulSoup(item.get_body_content(), "html.parser")

        # åˆ é™¤åˆ†é¡µ / Kindle æ®‹ç•™
        for tag in soup.find_all(["hr", "mbp:pagebreak"]):
            tag.decompose()

        for tag in soup.find_all(["h1", "h2", "h3", "p"]):
            txt = tag.get_text(" ", strip=True)
            if not txt:
                continue

            # ğŸ”¥ æ ¸å¿ƒè¿‡æ»¤é€»è¾‘ï¼ˆä½ è¦çš„é‚£æ®µï¼‰
            if is_bad_text(txt):
                continue
            # if txt in toc_texts:
            #     continue
            if "Table of Contents" in txt:
                continue
            if txt.startswith("æœ¬ä¹¦ç”±"):
                continue

            # ç« èŠ‚åˆ‡åˆ†
            if tag.name in ["h1", "h2"]:
                # ä¿å­˜ä¸Šä¸€ç« 
                if current_chapter_title or current_chapter_content:
                    chapters.append(
                        (current_chapter_title, "\n".join(current_chapter_content))
                    )

                current_chapter_title = txt
                current_chapter_content = [txt]  # æ ‡é¢˜ä¹Ÿæ”¾å…¥æ­£æ–‡
            else:
                current_chapter_content.append(txt)

    # æœ€åä¸€ç« 
    if current_chapter_title or current_chapter_content:
        chapters.append(
            (current_chapter_title, "\n".join(current_chapter_content))
        )

    # è¾“å‡ºç¤ºä¾‹
    for i, (title, content) in enumerate(chapters, start=1):
        print(f"\n--- ç¬¬ {i} ç« : {title} ---\n")
        print(content[:800])
        print("\n=========================\n")

    return chapters

def read_mobi(file_path):
    """è§£æ MOBIï¼ŒæŒ‰ç›®å½•é”šç‚¹è¾“å‡ºç« èŠ‚"""
    tempdir, filepath = mobi.extract(file_path)
    with open(filepath, "r", encoding="utf-8", errors="ignore") as f:
        html = f.read()

    print(f"æºæ–‡ä»¶ï¼š{file_path}ï¼Œè§£æåç›®å½•ï¼š{tempdir}, è§£æåæ–‡ä»¶ï¼š{filepath}")

    soup = BeautifulSoup(html, "html.parser")

    for pagebreak in soup.find_all("mbp:pagebreak"):
        pagebreak.decompose()  # ä»æ–‡æ¡£ä¸­åˆ é™¤

    # 1. æ‰¾ç›®å½•åŒºï¼ˆTable of Contentsï¼‰
    toc_tag = soup.find(string=lambda x: x and "Table of Contents" in x)
    if not toc_tag:
        print("âŒ æ²¡æ‰¾åˆ° TOCï¼Œå›é€€åˆ° <h1>/<h2> æ–¹å¼")
        return

    toc_section = toc_tag.find_parent("p")
    toc_links = toc_section.find_all_next("a", href=True)

    # 2. æ”¶é›†ç« èŠ‚é“¾æ¥
    chapters_info = []
    seen_hrefs = set()  # ç”¨æ¥è®°å½•å·²æ·»åŠ è¿‡çš„ href

    for a in toc_links:
        href = a["href"]
        if not href.startswith("#filepos"):
            print(f" å¼‚å¸¸çš„é“¾æ¥ï¼š{href} ")
            break # å¼‚å¸¸é€€å‡º
        if href not in seen_hrefs:
            chapters_info.append((a.get_text(strip=True), href[1:]))
            seen_hrefs.add(href)

    print(f"ğŸ“‘ æ‰¾åˆ° {len(chapters_info)} ä¸ªç« èŠ‚")

    # 3. æŒ‰é”šç‚¹åˆ‡æ­£æ–‡
    chapters = []
    toc_texts = set([a.get_text(strip=True) for a in toc_links])  # TOC çš„æ‰€æœ‰æ–‡å­—ï¼Œæ–¹ä¾¿è¿‡æ»¤
    for i, (title, anchor) in enumerate(chapters_info):
        start = soup.find("a", {"id": anchor})
        if not start:
            continue

        # æ‰¾ä¸‹ä¸€ä¸ªç« èŠ‚çš„èµ·ç‚¹
        end_anchor = chapters_info[i+1][1] if i+1 < len(chapters_info) else None
        texts = []
        node = start
        while node:
            node = node.find_next()
            if not node:
                break
            if end_anchor and node.name == "a" and node.get("id") == end_anchor:
                break
            if node.name in ["p", "h1", "h2", "h3"]:
                txt = node.get_text(" ", strip=True)
                # ğŸ”¥ è¿‡æ»¤å¹¿å‘Š è¿‡æ»¤æ‰ç›®å½•çš„æ–‡å­—
                if txt and (not is_bad_text(txt)) and (txt not in toc_texts and "Table of Contents" not in txt):  
                    texts.append(txt)

        chapters.append((title, "\n".join(texts)))

    # 4. è¾“å‡ºç»“æœ
    # print(f"=== {os.path.basename(file_path)} ===")
    # for i, (title, content) in enumerate(chapters, start=1):
    #     print(f"\n--- ç¬¬ {i} ç«  {title} ---\n")
    #     print(content[:100])  # æ˜¾ç¤ºå‰ 800 å­—
    #     print("\n=========================\n")

    shutil.rmtree(tempdir)
    return chapters


def read_book(file_path):
    """è‡ªåŠ¨åˆ¤æ–­æ–‡ä»¶ç±»å‹å¹¶è§£æ"""
    if file_path.endswith(".epub"):
        return read_epub(file_path)
    elif file_path.endswith(".mobi"):
        return read_mobi(file_path)
    elif file_path.endswith(".txt"):
        return
    else:
        print("âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼:", file_path)

def split_text_by_length(text, max_len=7200):
    """
    å°†æ–‡æœ¬æŒ‰ max_len åˆ†æ®µï¼Œæ¯æ®µå°½é‡åœ¨é åçš„æ¢è¡Œç¬¦å¤„åˆ†å‰²ã€‚
    """
    segments = []
    start = 0
    txt_len = len(text)
    parts = int(txt_len / max_len) + 1
    part_len  = int(txt_len / parts) + 1
    while start < txt_len:
        # å¦‚æœå‰©ä½™é•¿åº¦å°äºæœ€å¤§é•¿åº¦ï¼Œç›´æ¥åŠ å…¥
        if txt_len - start <= part_len:
            segments.append(text[start:].strip())
            break
        
        # é»˜è®¤åˆ‡ç‚¹
        split_pos = start + part_len

        newline_pos = text.find("\n", split_pos)
        if newline_pos != -1 and newline_pos > start:
            split_pos = newline_pos + 1  # åŒ…å«æ¢è¡Œç¬¦
        else:
            # å°è¯•å¾€åæ‰¾åˆ°ç¬¬ä¸€ä¸ªæ¢è¡Œç¬¦
            newline_pos = text.rfind("\n", 0, split_pos)
            if newline_pos != -1:
                split_pos = newline_pos + 1  # åŒ…å«æ¢è¡Œç¬¦
        
        # åˆ‡åˆ†
        segment = text[start:split_pos].strip()
        segments.append(segment)
        
        # æ›´æ–°èµ·ç‚¹
        start = split_pos

    return segments

def safe_filename(name: str) -> str:
    # æ›¿æ¢æ‰€æœ‰éæ³•æ–‡ä»¶åå­—ç¬¦
    for ch in ['\\', '/', ':', '*', '?', '"', '<', '>', '|']:
        name = name.replace(ch, '-')
    return name.strip()


def to_txt(file_path, txt_dir):

    ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
    file_path=os.path.abspath(os.path.join(ROOT_DIR, file_path))
    txt_dir=os.path.abspath(os.path.join(ROOT_DIR, txt_dir))
    
    chapters = read_book(file_path)
    if not chapters:
        return
    
    filename = os.path.splitext(os.path.basename(file_path))[0]
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = os.path.join(txt_dir, filename)
    os.makedirs(output_dir, exist_ok=True)

    for i, (title, content) in enumerate(chapters, start=1):
        # æ ‡é¢˜ä¸ºç©º
        if not title or title == content:
            continue
        title = title.strip()
        # è¿‡æ»¤ç›®å½• / å¯¼èˆª / åœ°æ ‡é¡µ
        if title in ("ç›®å½•", "Landmarks"):
            continue
        # è‹±æ–‡ç›®å½• / å˜ä½“
        if title.lower() in ("contents", "table of contents"):
            continue
        # ä¸€äº› EPUB å¸¸è§çš„æ— æ•ˆç« èŠ‚
        if title.startswith(("ç‰ˆæƒ", "å‰è¨€", "åº", "è‡´è°¢", "about", "copyright", "å°åº•")):
            continue
        # å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­
        # if not content or len(content.strip()) < 20:
        #     continue

        # æ›¿æ¢æ–‡æœ¬
        text = content.replace("â—‹", "é›¶")
        
        safe_title = safe_filename(title)
        segments = split_text_by_length(text)
        if len(segments) > 1:
            for j, p in enumerate(segments, 1):
                txt_content = f"Speaker 1: ç¬¬ {i} ç«  ç¬¬ {j} èŠ‚ \n{p}"
                txt_filename = f"{i:04d}.{j:02d}.{filename}.{safe_title}"

                output_path = os.path.join(output_dir, f"{txt_filename}.txt")
                with open(output_path, "w", encoding="utf-8") as f:
                    f.write(txt_content)

        else:
            txt_content = f"Speaker 1: ç¬¬ {i} ç«  {safe_title} \n{text}"
            txt_filename = f"{i:04d}.{filename}.{safe_title}"

            output_path = os.path.join(output_dir, f"{txt_filename}.txt")
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(txt_content)

# ç¤ºä¾‹è°ƒç”¨
if __name__ == "__main__":
    to_txt("./books/1996 ç»ˆæå®éªŒ - ç½—ä¼¯ç‰¹Â·ç´¢è€¶.epub", "./books/")
